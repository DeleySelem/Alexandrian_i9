import multilingus3 as ml3
import random
import difflib
import json
import os
import re
import time
import shp295  # Import shp295 module
from collections import defaultdict, Counter
import ml57 

class EnhancedLanguageGenerator(ml3.LanguageGenerator):
    def __init__(self):
        super().__init__()
        # Initialize shp295 components
        self.shp_nlp_processor = shp295.NeuroLinguisticProcessor()
        self.shp_neuromorph = shp295.NeuroMorphicProcessor()
        self.shp_hex_grid = shp295.HexagramGrid()
        self.shp_resources = [
            "words.json",
            "phrases.json",
            "mind.json",
            "parameters.json"
        ]
        
        self.corrections = defaultdict(list)
        self.last_original = None
        self.positive_tokens = set()
        self.negative_tokens = set()
        self.conversation_log = []
        self.token_analysis = defaultdict(lambda: defaultdict(int))
        self.BLUE_BOLD = "\033[1;34m"
        self.BLUE = "\033[34m"
        self.GREEN = "\033[32m"
        self.RED = "\033[31m"
        self.YELLOW = "\033[33m"
        self.RESET = "\033[0m"
        self.MACHINE_PROMPT = "\033[1;31m"
        self.MACHINE_RESPONSE = "\033[1;32m"
        self.load_resources()
        
    def load_resources(self):
        """Load all resources from files"""
        try:
            # Load corrections
            if os.path.exists('corrections.json'):
                with open('corrections.json', 'r') as f:
                    data = json.load(f)
                    for orig, corrs in data.items():
                        self.corrections[orig] = [(corr, weight) for corr, weight in corrs]
            
            # Load positive tokens
            if os.path.exists('plus.json'):
                with open('plus.json', 'r') as f:
                    try:
                        data = json.load(f)
                        if isinstance(data, list):
                            self.positive_tokens = set(data)
                        else:
                            print(f"{self.MACHINE_RESPONSE}Warning: plus.json has invalid format{self.RESET}")
                    except json.JSONDecodeError:
                        print(f"{self.MACHINE_RESPONSE}Error decoding plus.json{self.RESET}")
            
            # Load negative tokens
            if os.path.exists('minus.json'):
                with open('minus.json', 'r') as f:
                    try:
                        data = json.load(f)
                        if isinstance(data, list):
                            self.negative_tokens = set(data)
                        else:
                            print(f"{self.MACHINE_RESPONSE}Warning: minus.json has invalid format{self.RESET}")
                    except json.JSONDecodeError:
                        print(f"{self.MACHINE_RESPONSE}Error decoding minus.json{self.RESET}")
                    
            # Load conversation log
            if os.path.exists('conversation.log'):
                with open('conversation.log', 'r') as f:
                    self.conversation_log = [line.strip() for line in f.readlines()]
                    self.analyze_conversation()
                    
        except Exception as e:
            print(f"{self.MACHINE_RESPONSE}Error loading resources: {e}{self.RESET}")

    def save_resources(self):
        """Save all resources to files"""
        try:
            # Save corrections
            with open('corrections.json', 'w') as f:
                data = {orig: corrs for orig, corrs in self.corrections.items()}
                json.dump(data, f, indent=2)
                
            # Save positive tokens
            with open('plus.json', 'w') as f:
                json.dump(list(self.positive_tokens), f, indent=2)
                
            # Save negative tokens
            with open('minus.json', 'w') as f:
                json.dump(list(self.negative_tokens), f, indent=2)
                
            # Save conversation log
            with open('conversation.log', 'a') as f:
                for entry in self.conversation_log:
                    f.write(entry + "\n")
                    
        except Exception as e:
            print(f"{self.MACHINE_RESPONSE}Error saving resources: {e}{self.RESET}")

    def analyze_conversation(self):
        """Perform multifractal analysis on conversation log"""
        if not self.conversation_log:
            return
            
        text = " ".join(self.conversation_log).lower()
        
        # Analyze at different linguistic levels
        levels = {
            'letters': r'[a-z]',
            'pairs': r'[a-z]{2}',
            'triplets': r'[a-z]{3}',
            'words': r'\b\w+\b',
            'sentences': r'[^.!?]+'
        }
        
        for level, pattern in levels.items():
            tokens = re.findall(pattern, text)
            self.token_analysis[level] = Counter(tokens)

    def get_word_sentiment(self, word):
        """Calculate sentiment score for a word (-1 to 1)"""
        word_lower = word.lower()
        
        # Check exact matches
        if word_lower in self.positive_tokens:
            return 1.0
        if word_lower in self.negative_tokens:
            return -1.0
            
        # Check token-based sentiment
        pos_score, neg_score = 0, 0
        total_tokens = 0
        
        # Analyze tokens of different sizes
        for n in range(1, len(word_lower)+1):
            for i in range(len(word_lower) - n + 1):
                token = word_lower[i:i+n]
                total_tokens += 1
                if token in self.positive_tokens:
                    pos_score += 1
                if token in self.negative_tokens:
                    neg_score += 1
        
        # Calculate overall sentiment
        if total_tokens == 0:
            return 0.0
        return (pos_score - neg_score) / total_tokens

    def typewrite_text(self, text, delay=0.05):
        """Print text with typewriter effect and sentiment coloring"""
        if not text:
            return
            
        # Ensure proper sentence formatting
        text = text.strip()
        if not text[0].isupper():
            text = text[0].upper() + text[1:]
        if not any(text.endswith(p) for p in ['.', '!', '?']):
            text += '.'
            
        words = text.split()
        
        # Print with typewriter effect
        print(f"{self.MACHINE_PROMPT}> ", end='', flush=True)
        for i, word in enumerate(words):
            # Determine word color based on sentiment
            sentiment = self.get_word_sentiment(word)
            if sentiment > 0.3:
                color_code = self.GREEN
            elif sentiment < -0.3:
                color_code = self.RED
            else:
                color_code = self.YELLOW
                
            # Print word with delay
            end_char = ' ' if i < len(words)-1 else '\n'
            print(f"{color_code}{word}{self.RESET}", end=end_char, flush=True)
            time.sleep(delay)

    def update_sentiment_resources(self, text, is_positive):
        """Update sentiment resources based on user input"""
        words = re.findall(r'\b\w+\b', text.lower()) #convert to lowercase
        
        for word in words:
            # Add tokens of different sizes
            for n in range(1, len(word)+1):
                for i in range(len(word) - n + 1):
                    token = word[i:i+n]
                    if is_positive:
                        self.positive_tokens.add(token)
                    else:
                        self.negative_tokens.add(token)
         
        # Save updated resources
        self.save_resources()

    def print_heatmap(self, user_input, responses):
        """Print word heatmap for verbosity level 99"""
        # Collect all words
        all_words = user_input.split()
        for response in responses:
            all_words.extend(response.split())
        
        # Calculate frequencies
        word_freq = Counter(word.lower() for word in all_words)
        if not word_freq:
            return
            
        max_freq = max(word_freq.values())
        
        # Print header
        print(f"{self.BLUE_BOLD}Subconscious Processes: {self.RESET}", end='', flush=True)
        
        # Print words with color gradient
        for word, freq in word_freq.most_common():
            norm_freq = freq / max_freq
            
            # Color gradient: dark blue (0) -> blue (0.33) -> light blue (0.66) -> white (1.0)
            if norm_freq < 0.33:
                blue_val = 128 + int(127 * (norm_freq * 3))
                color_code = f"\033[38;2;0;0;{blue_val}m"
            elif norm_freq < 0.66:
                intensity = int(128 * (norm_freq - 0.33) * 3)
                color_code = f"\033[38;2;{intensity};{intensity};255m"
            else:
                intensity = 128 + int(127 * (norm_freq - 0.66) * 3)
                color_code = f"\033[38;2;{intensity};{intensity};255m"
            
            print(f"{color_code}{word}{self.RESET} ", end='', flush=True)
            time.sleep(0.05)
        print()

    def process_input(self, user_input):
        # Generate shp295 response in background
        shp_response = shp295.generate_response(
            user_input,
            self.shp_nlp_processor,
            self.shp_neuromorph,
            self.shp_hex_grid,
            self.shp_resources
        )
        
        # Combine shp295 response with user input (full length)
        combined_input = f"{shp_response} | {user_input}"
        
        # Process combined input through ML57
        self._process_combined_input(combined_input)

    def _process_combined_input(self, user_input):
        """Process the combined input through ML57 pipeline"""
        self.universe.process_parameters()
        self.conversation_log.append(f"User: {user_input}")
        
        # Handle question responses
        if user_input.endswith('?'):
            # Update linguistic analysis
            words = re.findall(r'\b\w+\b', user_input.lower())
            for word in words:
                # Update word frequencies
                self.token_analysis['words'][word] += 1
                self.token_analysis['question_words'][word] += 1
            
            # Automatically detect question words (language-agnostic)
            question_words = []
            if self.token_analysis['question_words']:
                total_questions = sum(self.token_analysis['question_words'].values())
                for word, count in self.token_analysis['question_words'].items():
                    # Calculate distinctiveness: frequency in questions vs general frequency
                    general_freq = self.token_analysis['words'].get(word, 1)
                    distinctiveness = count / (general_freq + 1)  # Avoid division by zero
                    
                    # Detect words that appear primarily in questions
                    if distinctiveness > 0.7 and count > total_questions * 0.1:
                        question_words.append(word)
            
            # Identify content words from current question
            content_words = []
            for word in words:
                # Skip short words and high-frequency question words
                if len(word) > 2 and word not in question_words:
                    content_words.append(word)
            
            # Generate response based on detected patterns
            if content_words:
                # Select up to 2 content words to reference
                selected_words = content_words[:2]
                responses = [
                    f"Associating: {', '.join(selected_words)}",
                    f"Connecting question patterns: {', '.join(selected_words)}",
                    f"Detected concepts: {', '.join(selected_words)}",
                    f"?: association with: {', '.join(selected_words)}",
                    f"Pattern match: {', '.join(selected_words)}"
                ]
            elif question_words:
                # Reference detected question words
                responses = [
                    f"Recognized question structure: {question_words[0]}",
                    f"Associating question pattern: {question_words[0]}",
                    f"Detected question word: {question_words[0]}",
                    f"?: pattern: {question_words[0]}",
                    f"Question association: {question_words[0]}"
                ]
            else:
                # Generic response if no patterns detected
                responses = [
                    "Associating question structure.",
                    "Detected question pattern.",
                    "?: connecting concepts.",
                    "Recognizing question format.",
                    "Associating question markers."
                ]
            
            response = random.choice(responses)
            self.conversation_log.append(f"System: {response}")
            self.typewrite_text(response)
            self.auto_input_counter += 1
            return
            
        words = user_input.split()
        is_negation = False
        negation_words = ['No', 'no', 'Ei', 'ei', 'Nein', 'nein', 'Non', 'non']
        
        # Check for negation commands
        if words and words[0] in negation_words:
            is_negation = True
            correction_phrase = ' '.join(words[1:])
            
            if self.last_original:
                # Update existing or add new correction
                updated = False
                new_corrections = []
                total_weight = 0
                
                for corr, weight in self.corrections[self.last_original]:
                    if corr == correction_phrase:
                        new_weight = weight + 0.1
                        updated = True
                    else:
                        new_weight = weight
                    new_corrections.append((corr, new_weight))
                    total_weight += new_weight
                
                if not updated:
                    new_corrections.append((correction_phrase, 1))
                    total_weight += 1
                
                # Normalize weights to percentages
                self.corrections[self.last_original] = [
                    (corr, weight/total_weight) 
                    for corr, weight in new_corrections
                ]
                
                # Build response
                response = "Correction stored."
                
                # Add correction details
                if self.universe.parameters["verbosity"] > 0:
                    response += f"\nOriginal: {self.last_original}"
                    response += f"\nCorrection: {correction_phrase}"
                
                # Update sentiment resources
                self.update_sentiment_resources(self.last_original, is_positive=False)
                self.update_sentiment_resources(correction_phrase, is_positive=True)
                
                # Print response (fixed Japanese symbol issue)
                self.conversation_log.append(f"System: {response}")
                self.typewrite_text(response)
                
                # Save and update
                self.save_resources()
                self.universe.update_with_sentence(user_input)
                self.auto_input_counter += 1
                return
                
        # Update language model
        if ' ' in user_input:
            self.universe.update_with_sentence(user_input)
        else:
            self.universe.update_with_word(user_input)
            
        # Store original phrase for potential correction
        if not is_negation:
            self.last_original = user_input
        
        # Generate responses
        responses = [self.universe.generate_sentence() for _ in range(37)]
        weights = [1.0] * 37
        
        # Apply corrections
        matching_corrections = []
        if not is_negation and self.corrections:
            for orig, corrs in self.corrections.items():
                char_sim = self.char_sequence_similarity(user_input, orig)
                pair_sim = self.char_pair_similarity(user_input, orig)
                triplet_sim = self.char_triplet_similarity(user_input, orig)
                word_sim = self.word_sequence_similarity(user_input, orig)
                phrase_sim = self.phrase_similarity(user_input, orig)
                
                combined_sim = (
                    0.15 * char_sim + 
                    0.20 * pair_sim + 
                    0.25 * triplet_sim + 
                    0.20 * word_sim + 
                    0.20 * phrase_sim
                )
                
                if combined_sim > 0.4:
                    for corr, weight in corrs:
                        candidate_weight = combined_sim * weight * 1000
                        matching_corrections.append((corr, candidate_weight))
                        
                        # Verbose output
                        if self.universe.parameters["verbosity"] > 20 and self.universe.parameters["verbosity"] != 99:
                            details = (
                                f"Match: '{orig}' → '{corr}' "
                                f"(char:{char_sim:.2f} pair:{pair_sim:.2f} "
                                f"triplet:{triplet_sim:.2f} word:{word_sim:.2f} "
                                f"phrase:{phrase_sim:.2f} combined:{combined_sim:.2f} "
                                f"weight:{candidate_weight:.1f})"
                            )
                            print(f"{self.BLUE_BOLD}Subconscious Processes: {self.BLUE}{details}{self.RESET}")
        
        for corr, weight in matching_corrections:
            responses.append(corr)
            weights.append(weight)
            
        # Select response
        total_weight = sum(weights)
        if total_weight > 0:
            normalized_weights = [w/total_weight for w in weights]
            selected_index = random.choices(range(len(responses)), weights=normalized_weights, k=1)[0]
        else:
            selected_index = random.randint(0, len(responses)-1)
            
        selected_response = responses[selected_index]
        
        # Handle verbosity level 99
        if self.universe.parameters["verbosity"] == 99:
            self.print_heatmap(user_input, responses)
        
        # Handle other verbosity levels
        elif self.universe.parameters["verbosity"] > 30:
            if selected_index >= len(responses) - len(matching_corrections):
                orig = next(orig for orig, corrs in self.corrections.items() 
                           if any(corr == selected_response for corr, _ in corrs))
                verbose = f"Selected correction: '{orig}' → '{selected_response}'"
            else:
                verbose = f"Selected generated response: '{selected_response}'"
            
            print(f"{self.BLUE_BOLD}Subconscious Processes: {self.BLUE}{verbose}{self.RESET}")
        
        # Print final response
        self.conversation_log.append(f"System: {selected_response}")
        self.typewrite_text(selected_response)
        self.auto_input_counter += 1
        
        # Update analysis and save
        self.analyze_conversation()
        self.save_resources()

    # Similarity methods
    def char_sequence_similarity(self, s1, s2):
        return difflib.SequenceMatcher(None, s1.lower(), s2.lower()).ratio()
    
    def char_pair_similarity(self, s1, s2):
        pairs1 = set(s1[i:i+2] for i in range(len(s1)-1))
        pairs2 = set(s2[i:i+2] for i in range(len(s2)-1))
        return len(pairs1 & pairs2) / max(len(pairs1 | pairs2), 1)
    
    def char_triplet_similarity(self, s1, s2):
        triplets1 = set(s1[i:i+3] for i in range(len(s1)-2))
        triplets2 = set(s2[i:i+3] for i in range(len(s2)-2))
        return len(triplets1 & triplets2) / max(len(triplets1 | triplets2), 1)
    
    def word_sequence_similarity(self, s1, s2):
        words1 = s1.lower().split()
        words2 = s2.lower().split()
        return difflib.SequenceMatcher(None, words1, words2).ratio()
    
    def phrase_similarity(self, s1, s2):
        seq_match = difflib.SequenceMatcher(None, s1.lower(), s2.lower()).ratio()
        words1 = set(s1.lower().split())
        words2 = set(s2.lower().split())
        jaccard = len(words1 & words2) / len(words1 | words2) if words1 or words2 else 0.0
        return (seq_match * 0.7) + (jaccard * 0.3)

if __name__ == "__main__":
    generator = EnhancedLanguageGenerator()
    generator.start()
